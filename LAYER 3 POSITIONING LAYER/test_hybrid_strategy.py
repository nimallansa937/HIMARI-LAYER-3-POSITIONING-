"""
HIMARI Layer 3:  Hybrid Strategy (FIXED v2.1)
=============================================

Fixes from v2.0:
- Consistent hourly annualization (sqrt(252*24) = 77.76)
- Proper regime thresholds for hourly data
- Correct Sharpe ratio calculations
- Fixed import paths
- Added drawdown tracking
- Added commission costs

Version:  2.1 (Corrected)
"""

import os
import sys

# Fix import path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

import time
import numpy as np
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from enum import Enum

import torch
import torch.nn as nn

# =============================================================================
# CONSTANTS
# =============================================================================

# Annualization factor for hourly data
HOURLY_ANNUALIZATION = np.sqrt(252 * 24)  # ~77.76
COMMISSION_RATE = 0.001  # 0.1%


# =============================================================================
# Configuration
# =============================================================================

@dataclass
class TrainingConfig:
    """Training configuration - must match saved models."""
    state_dim: int = 16
    hidden_dim: int = 128
    lstm_layers: int = 2
    dropout: float = 0.2
    learning_rate: float = 3e-4
    gamma: float = 0.99
    lambda_gae: float = 0.95
    clip_epsilon: float = 0.2
    value_coef: float = 0.5
    entropy_coef: float = 0.01
    max_grad_norm: float = 0.5
    checkpoint_interval: int = 50000
    patience: int = 3
    initial_capital: float = 100000.0
    max_position_pct: float = 0.5
    commission_rate: float = 0.001


# Make available for model loading
import __main__
__main__.TrainingConfig = TrainingConfig


class MarketRegime(Enum):
    """Market regime classification."""
    NORMAL = "NORMAL"
    HIGH_VOL = "HIGH_VOL"
    CRISIS = "CRISIS"
    CASCADE = "CASCADE"


# =============================================================================
# Neural Network
# =============================================================================

class LSTMPPONetworkV2(nn.Module):
    """LSTM-PPO network matching training architecture."""
    
    def __init__(self, config: TrainingConfig):
        super().__init__()
        self.config = config
        self.input_proj = nn.Sequential(
            nn.Linear(config.state_dim, config.hidden_dim),
            nn.LayerNorm(config.hidden_dim),
            nn.ReLU(),
            nn.Dropout(config.dropout)
        )
        self.lstm = nn.LSTM(
            config.hidden_dim, config.hidden_dim, config.lstm_layers,
            batch_first=True,
            dropout=config.dropout if config.lstm_layers > 1 else 0
        )
        self.actor = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.hidden_dim // 2, 2)
        )
        self.critic = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.hidden_dim // 2, 1)
        )
    
    def forward(self, x):
        if x.dim() == 2:
            x = x.unsqueeze(1)
        x = self.input_proj(x)
        out, _ = self.lstm(x)
        return self.actor(out[:, -1, :])[:, 0:1]


def load_model(path: str) -> nn.Module:
    """Load model from checkpoint."""
    checkpoint = torch.load(path, map_location='cpu', weights_only=False)
    config = checkpoint.get('config', TrainingConfig())
    model = LSTMPPONetworkV2(config)
    model.load_state_dict(checkpoint['network_state_dict'], strict=True)
    model.eval()
    return model


def predict_rl_raw(model: nn.Module, state: np.ndarray) -> float:
    """Get RAW RL output (before bounding)."""
    with torch.no_grad():
        state_t = torch.FloatTensor(state).unsqueeze(0)
        action = model(state_t)
        return torch.tanh(action).item()


# =============================================================================
# Bounded Delta Position Sizing
# =============================================================================

@dataclass
class BoundedDeltaConfig:
    """Configuration for bounded delta position sizing."""
    max_rl_delta: float = 0.30
    target_volatility: float = 0.15  # 15% annualized
    vol_lookback: int = 20
    leverage_caps: Dict[str, float] = field(default_factory=lambda: {
        "NORMAL": 2.0,
        "HIGH_VOL": 1.5,
        "CRISIS": 1.0,
        "CASCADE": 0.0
    })


class BoundedDeltaPositionSizer: 
    """
    Bounded delta position sizing with volatility targeting.
    
    RL output is bounded to ¬±30% adjustment from vol-targeting base.
    """
    
    def __init__(self, config:  BoundedDeltaConfig = None):
        self.config = config or BoundedDeltaConfig()
        self.returns_history:  List[float] = []
    
    def update_returns(self, return_value: float):
        """Track returns for volatility calculation."""
        self.returns_history.append(return_value)
        if len(self.returns_history) > 100:
            self.returns_history.pop(0)
    
    def calculate_volatility_target_position(self, capital: float) -> float:
        """Calculate base position using volatility targeting."""
        if len(self.returns_history) < self.config.vol_lookback:
            return capital * 0.25  # Conservative default
        
        # Realized volatility (HOURLY annualization)
        realized_vol = np.std(self.returns_history[-self.config.vol_lookback:]) * HOURLY_ANNUALIZATION
        
        # Floor volatility
        realized_vol = max(realized_vol, 0.05)  # Min 5% annualized
        
        # Vol scalar
        vol_scalar = self.config.target_volatility / realized_vol
        vol_scalar = np.clip(vol_scalar, 0.1, 3.0)
        
        # Base position (max 50% of capital)
        base_position = capital * 0.02 * vol_scalar  # 2% base fraction
        base_position = min(base_position, capital * 0.5)
        
        return base_position
    
    def apply_bounded_delta(
        self,
        raw_rl_output: float,
        capital: float,
        regime:  MarketRegime
    ) -> Tuple[float, Dict]: 
        """Apply bounded delta constraint with regime gating."""
        
        # Step 1: Get base position
        base_position = self.calculate_volatility_target_position(capital)
        
        # Step 2: Clip and scale RL output
        clipped_rl = np.clip(raw_rl_output, -1.0, 1.0)
        rl_delta = clipped_rl * self.config.max_rl_delta
        
        # Step 3: Apply delta
        adjusted_position = base_position * (1.0 + rl_delta)
        
        # Step 4: Regime gating
        leverage_cap = self.config.leverage_caps.get(regime.value, 1.0)
        max_position = capital * leverage_cap
        
        rl_disabled = regime in [MarketRegime.CRISIS, MarketRegime.CASCADE]
        
        if rl_disabled:
            final_position = min(base_position * 0.5, max_position)  # Extra conservative
        else:
            final_position = min(adjusted_position, max_position)
        
        final_position = max(0, final_position)
        
        debug_info = {
            'base_position': base_position,
            'raw_rl_output': raw_rl_output,
            'clipped_rl': clipped_rl,
            'rl_delta': rl_delta,
            'adjusted_position': adjusted_position,
            'regime': regime.value,
            'leverage_cap': leverage_cap,
            'rl_disabled': rl_disabled,
            'final_position':  final_position,
        }
        
        return final_position, debug_info


# =============================================================================
# Regime Detection (FIXED:  Consistent annualization)
# =============================================================================

class RegimeDetector: 
    """
    Detect market regime using LAGGED indicators only.
    
    FIXED: Uses consistent hourly annualization. 
    """
    
    def __init__(self, lookback: int = 50):
        self.lookback = lookback
        self.returns_history: List[float] = []
        self.volatility_history: List[float] = []
    
    def update(self, return_value: float):
        """Update with new return."""
        self.returns_history.append(return_value)
        
        if len(self.returns_history) >= 20:
            # FIXED: Use hourly annualization
            vol = np.std(self.returns_history[-20:]) * HOURLY_ANNUALIZATION
            self.volatility_history.append(vol)
        
        # Keep bounded history
        if len(self.returns_history) > self.lookback * 2:
            self.returns_history.pop(0)
        if len(self.volatility_history) > self.lookback:
            self.volatility_history.pop(0)
    
    def detect(self) -> Tuple[str, MarketRegime]:
        """Detect regime using ONLY past data."""
        if len(self.returns_history) < self.lookback:
            return "neutral", MarketRegime.NORMAL
        
        # Momentum metrics
        returns_20 = np.sum(self.returns_history[-20:])
        returns_50 = np.sum(self.returns_history[-self.lookback:])
        
        # Volatility metrics
        current_vol = self.volatility_history[-1] if self.volatility_history else 0.3
        avg_vol = np.mean(self.volatility_history[-10:]) if len(self.volatility_history) >= 10 else current_vol
        
        # FIXED: Thresholds appropriate for hourly-annualized vol
        # BTC typically:  Normal ~40-60%, High Vol ~60-100%, Crisis >100%
        if current_vol > 1.2:  # >120% annualized
            return "neutral", MarketRegime.CASCADE
        elif current_vol > 0.8:  # >80% annualized
            return "neutral", MarketRegime.CRISIS
        elif current_vol > 0.6:  # >60% annualized
            market_regime = MarketRegime.HIGH_VOL
        else: 
            market_regime = MarketRegime.NORMAL
        
        # Trend direction
        momentum_score = returns_20 * 0.6 + returns_50 * 0.4
        
        if momentum_score > 0.03:  # >3% positive (adjusted for hourly)
            return "bull", market_regime
        elif momentum_score < -0.03:
            return "bear", market_regime
        else:
            return "neutral", market_regime


# =============================================================================
# Feature Engineering (FIXED: Added drawdown tracking)
# =============================================================================

class HybridFeatureEngineer:
    """Feature engineering with proper state construction."""
    
    def __init__(self, initial_capital: float = 100000.0):
        self.price_history: List[float] = []
        self.returns_history: List[float] = []
        self.peak_capital = initial_capital
        self.max_history = 500
    
    def update(self, price: float):
        """Update with new price."""
        if self.price_history:
            ret = (price - self.price_history[-1]) / self.price_history[-1]
            self.returns_history.append(ret)
            if len(self.returns_history) > self.max_history:
                self.returns_history.pop(0)
        
        self.price_history.append(price)
        if len(self.price_history) > self.max_history:
            self.price_history.pop(0)
    
    def get_state(
        self,
        current_position_usd: float,
        current_capital: float,
        initial_capital: float,
        regime_name: str
    ) -> np.ndarray:
        """
        Get 16-dim state vector. 
        
        FIXED:  Proper drawdown tracking and regime mapping.
        """
        state = np.zeros(16, dtype=np.float32)
        
        if len(self.returns_history) < 20:
            return state
        
        # Update peak capital for drawdown
        self.peak_capital = max(self.peak_capital, current_capital)
        current_drawdown = (current_capital - self.peak_capital) / self.peak_capital
        
        # Total PnL
        total_pnl_pct = (current_capital - initial_capital) / initial_capital
        
        # Momentum calculations
        momentum_1h = np.sum(self.returns_history[-60:]) if len(self.returns_history) >= 60 else np.sum(self.returns_history)
        momentum_4h = np.sum(self.returns_history[-240:]) if len(self.returns_history) >= 240 else momentum_1h
        
        # Volatility
        volatility = np.std(self.returns_history[-20:]) * HOURLY_ANNUALIZATION
        
        # RSI proxy
        recent = self.returns_history[-14:]
        gains = [r for r in recent if r > 0]
        losses = [-r for r in recent if r < 0]
        avg_gain = np.mean(gains) if gains else 0
        avg_loss = np.mean(losses) if losses else 1e-8
        rsi = 100 - (100 / (1 + avg_gain / (avg_loss + 1e-8)))
        
        # Map regime to numeric
        regime_map = {"bull": 0, "bear": 1, "neutral": 2}
        regime_encoded = regime_map.get(regime_name, 2) / 4.0
        
        # Build state vector (matching StateEncoder format)
        # 1. Signal confidence
        state[0] = 0.75
        
        # 2-4. Signal action one-hot (BUY)
        state[1] = 0  # SELL
        state[2] = 0  # HOLD
        state[3] = 1  # BUY
        
        # 5-8. Tier one-hot (T1)
        state[4] = 1  # T1
        state[5] = 0  # T2
        state[6] = 0  # T3
        state[7] = 0  # T4
        
        # 9. Position size (normalized)
        max_position = initial_capital * 0.5
        state[8] = min(abs(current_position_usd) / max_position, 1.0)
        
        # 10. Position side
        state[9] = 1 if current_position_usd > 0 else (-1 if current_position_usd < 0 else 0)
        
        # 11. Unrealized P&L (simplified to total P&L)
        state[10] = np.clip(total_pnl_pct * 2, -1, 1)
        
        # 12. Momentum 1h
        state[11] = np.clip(momentum_1h * 10, -1, 1)
        
        # 13. Momentum 4h
        state[12] = np.clip(momentum_4h * 5, -1, 1)
        
        # 14. Volatility
        state[13] = np.clip(volatility / 0.1, 0, 1)
        
        # 15. Recent win rate (default 0.5)
        state[14] = 0.5
        
        # 16. Cascade risk (from regime)
        cascade_risk_map = {"bull": 0.1, "bear": 0.4, "neutral": 0.2}
        state[15] = cascade_risk_map.get(regime_name, 0.2)
        
        return state


# =============================================================================
# Kelly/Momentum Strategy (FIXED: Better edge cases)
# =============================================================================

def kelly_momentum_position(
    returns: List[float],
    capital: float,
    max_position_pct: float = 0.5
) -> float:
    """Kelly criterion + momentum for bull markets."""
    if len(returns) < 20:
        return capital * 0.25
    
    recent_returns = returns[-20:]
    
    wins = [r for r in recent_returns if r > 0]
    losses = [r for r in recent_returns if r < 0]
    
    # FIXED: Handle edge cases properly
    if not losses:
        # All wins - be conservative, not aggressive
        kelly = 0.25
    elif not wins:
        # All losses - minimal position
        kelly = 0.05
    else:
        win_rate = len(wins) / len(recent_returns)
        avg_win = np.mean(wins)
        avg_loss = abs(np.mean(losses))
        
        # Kelly fraction
        if avg_loss > 0:
            kelly = win_rate - (1 - win_rate) / (avg_win / avg_loss)
        else:
            kelly = 0.25
        
        kelly = np.clip(kelly, 0, 0.5)
    
    # Momentum factor
    momentum = np.sum(recent_returns)
    momentum_factor = 1 + np.clip(momentum * 2, -0.3, 0.3)
    
    # Position
    position = capital * kelly * momentum_factor * max_position_pct
    
    return min(position, capital * max_position_pct)


# =============================================================================
# Main Backtest (FIXED: Consistent annualization, commissions)
# =============================================================================

def run_hybrid_backtest(
    models: List[nn.Module],
    prices: np.ndarray,
    initial_capital: float = 100000.0
) -> Dict:
    """Run hybrid strategy backtest with all fixes applied."""
    
    # Initialize components
    regime_detector = RegimeDetector(lookback=50)
    position_sizer = BoundedDeltaPositionSizer()
    feature_eng = HybridFeatureEngineer(initial_capital=initial_capital)
    
    # Tracking
    capital = initial_capital
    position_usd = 0.0
    prev_position_pct = 0.0
    returns_list = []
    regime_counts = {"bull": 0, "bear": 0, "neutral": 0}
    rl_disabled_count = 0
    total_commission = 0.0
    
    for t in range(1, len(prices)):
        # Calculate return
        ret = (prices[t] - prices[t-1]) / prices[t-1]
        
        # Progress update
        if t % 1752 == 0:  # ~10% intervals for 2yr hourly
            pct = t / len(prices) * 100
            regime_name, _ = regime_detector.detect()
            print(f"   ‚è≥ {pct:.0f}% | Capital: ${capital:,.0f} | Regime: {regime_name.upper()}")
        
        # Update all components with this period's data
        regime_detector.update(ret)
        position_sizer.update_returns(ret)
        feature_eng.update(prices[t])
        
        # Warmup period
        if t < 60:
            returns_list.append(0)
            continue
        
        # Detect regime
        regime_name, market_regime = regime_detector.detect()
        regime_counts[regime_name] += 1
        
        # Get state for RL
        state = feature_eng.get_state(
            current_position_usd=position_usd,
            current_capital=capital,
            initial_capital=initial_capital,
            regime_name=regime_name
        )
        
        # Strategy selection
        if regime_name == "bull":
            target_position = kelly_momentum_position(
                feature_eng.returns_history,
                capital
            )
        else:
            # RL Ensemble with bounded delta
            rl_outputs = [predict_rl_raw(model, state) for model in models]
            ensemble_output = np.mean(rl_outputs)
            
            target_position, debug = position_sizer.apply_bounded_delta(
                raw_rl_output=ensemble_output,
                capital=capital,
                regime=market_regime
            )
            
            if debug['rl_disabled']:
                rl_disabled_count += 1
        
        # Calculate position change and commission
        position_pct = target_position / capital if capital > 0 else 0
        position_change = abs(position_pct - prev_position_pct)
        commission = position_change * COMMISSION_RATE
        total_commission += commission * capital
        
        # Calculate strategy return (with commission)
        strategy_return = position_pct * ret - commission
        
        # Update state
        capital *= (1 + strategy_return)
        position_usd = target_position
        prev_position_pct = position_pct
        returns_list.append(strategy_return)
    
    # Calculate metrics
    returns_arr = np.array(returns_list)
    
    # FIXED: Use hourly annualization
    sharpe = np.mean(returns_arr) / (np.std(returns_arr) + 1e-8) * HOURLY_ANNUALIZATION
    total_return = (capital - initial_capital) / initial_capital
    
    # Max drawdown
    cumulative = np.cumsum(returns_arr)
    running_max = np.maximum.accumulate(cumulative)
    drawdowns = running_max - cumulative
    max_dd = np.max(drawdowns)
    
    total_periods = sum(regime_counts.values())
    
    return {
        'sharpe': sharpe,
        'total_return': total_return,
        'max_drawdown': max_dd,
        'final_capital': capital,
        'total_commission': total_commission,
        'regime_distribution': {
            k: v / total_periods for k, v in regime_counts.items()
        },
        'rl_disabled_pct': rl_disabled_count / total_periods if total_periods > 0 else 0,
    }


# =============================================================================
# Main
# =============================================================================

def main():
    print("=" * 70)
    print("HIMARI Layer 3:  HYBRID STRATEGY (FIXED v2. 1)")
    print("=" * 70)
    print()
    print("FIXES IN v2.1:")
    print("  ‚úÖ Consistent hourly annualization (‚àö(252√ó24) = 77.76)")
    print("  ‚úÖ Regime thresholds adjusted for hourly volatility")
    print("  ‚úÖ Commission costs included")
    print("  ‚úÖ Drawdown tracking in state")
    print("  ‚úÖ Kelly edge cases handled")
    print()
    
    print("üéØ Strategy:")
    print("   Bull Market ‚Üí Kelly/Momentum (aggressive)")
    print("   Bear Market ‚Üí RL Ensemble with BOUNDED DELTA")
    print("   Crisis      ‚Üí PURE VOL-TARGETING (RL disabled)")
    print()
    
    # Load models
    print("Loading RL ensemble models...")
    models_dir = "C:/Users/chari/OneDrive/Documents/HIMARI OPUS 2/LAYER 3 V1/LAYER 3 TRAINED ESSEMBLE MODLES - Copy"
    model_files = [f"model_{i}.pt" for i in range(1, 6)]
    
    models = []
    for f in model_files:
        path = os.path.join(models_dir, f)
        if os.path.exists(path):
            try:
                model = load_model(path)
                models.append(model)
                print(f"   ‚úÖ {f}")
            except Exception as e: 
                print(f"   ‚ùå {f}: {e}")
        else:
            print(f"   ‚ö†Ô∏è {f} not found")
    
    if not models:
        print("\n‚ö†Ô∏è No models loaded, using dummy models")
        models = [LSTMPPONetworkV2(TrainingConfig()) for _ in range(5)]
    
    # Generate test data
    print("\nüìä Generating realistic market data (GARCH + Jump-Diffusion)...")
    np.random.seed(42)
    
    n_days = 730  # 2 years
    n_candles = n_days * 24  # Hourly
    
    # GARCH parameters for hourly data
    omega, alpha, beta = 4e-7, 0.10, 0.85
    lambda_jump, mu_jump, sigma_jump = 0.02, -0.03, 0.05
    
    returns = np.zeros(n_candles)
    sigma2 = np.zeros(n_candles)
    sigma2[0] = omega / (1 - alpha - beta)
    
    for t in range(1, n_candles):
        sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]
        returns[t] = np.random.normal(0, np.sqrt(sigma2[t]))
        
        if np.random.random() < lambda_jump / 24:
            returns[t] += np.random.normal(mu_jump, sigma_jump)
    
    prices = 50000 * np.exp(np.cumsum(returns))
    print(f"   ‚úÖ Generated {len(prices):,} hourly candles ({n_days} days)")
    
    # Run backtest
    print("\n" + "=" * 70)
    print("BACKTESTING HYBRID STRATEGY")
    print("=" * 70)
    
    results = run_hybrid_backtest(models, prices)
    
    # Buy & Hold comparison (FIXED: correct annualization)
    bh_return = (prices[-1] - prices[0]) / prices[0]
    bh_returns = np.diff(prices) / prices[:-1]
    bh_sharpe = np.mean(bh_returns) / (np.std(bh_returns) + 1e-8) * HOURLY_ANNUALIZATION
    
    cumulative_bh = np.cumsum(bh_returns)
    running_max_bh = np.maximum.accumulate(cumulative_bh)
    bh_max_dd = np.max(running_max_bh - cumulative_bh)
    
    # Results
    print("\n" + "=" * 70)
    print("RESULTS")
    print("=" * 70)
    
    print(f"\n{'Metric':<35} {'Hybrid':<15} {'Buy & Hold':<15}")
    print("-" * 65)
    print(f"{'Sharpe Ratio':<35} {results['sharpe']:+.4f}       {bh_sharpe:+.4f}")
    print(f"{'Total Return':<35} {results['total_return']*100:+.2f}%        {bh_return*100:+.2f}%")
    print(f"{'Max Drawdown':<35} {results['max_drawdown']*100:.2f}%         {bh_max_dd*100:.2f}%")
    print(f"{'Final Capital':<35} ${results['final_capital']:,.0f}")
    print(f"{'Total Commission':<35} ${results['total_commission']:,.0f}")
    
    print(f"\nüìä Regime Distribution:")
    for regime, pct in results['regime_distribution'].items():
        print(f"   {regime.capitalize():<10} {pct*100:>6.1f}%")
    
    print(f"\nüõ°Ô∏è RL Disabled (Crisis): {results['rl_disabled_pct']*100:.1f}%")
    
    # Verdict
    print("\n" + "=" * 70)
    print("VERDICT")
    print("=" * 70)
    
    if results['sharpe'] > 0:
        print(f"\n‚úÖ Hybrid Sharpe:  {results['sharpe']:+.4f} (POSITIVE)")
    else:
        print(f"\n‚ö†Ô∏è Hybrid Sharpe: {results['sharpe']:+.4f} (NEGATIVE)")
    
    if results['sharpe'] > bh_sharpe:
        diff = results['sharpe'] - bh_sharpe
        print(f"üéØ BEATS Buy & Hold by {diff:+.4f} Sharpe points!")
    else:
        diff = bh_sharpe - results['sharpe']
        print(f"üìâ Underperforms Buy & Hold by {diff:.4f} Sharpe points")
    
    print("=" * 70)


if __name__ == "__main__":
    main()
