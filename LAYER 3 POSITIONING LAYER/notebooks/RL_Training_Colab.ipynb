{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIMARI OPUS 2 - RL Position Sizing Training\n",
    "\n",
    "Train PPO agent to optimize position sizing using live market data.\n",
    "\n",
    "**Features:**\n",
    "- Live price feed from Binance API\n",
    "- Paper trading environment\n",
    "- PPO reinforcement learning\n",
    "- Model checkpointing\n",
    "- Performance visualization\n",
    "\n",
    "**Runtime:** Google Colab (CPU or GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy requests matplotlib tqdm\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone HIMARI Layer 3 from GitHub\n!git clone https://github.com/nimallansa937/HIMARI-LAYER-3-POSITIONING-.git\n%cd HIMARI-LAYER-3-POSITIONING-\n\nprint(\"‚úÖ HIMARI Layer 3 cloned from GitHub\")\nprint(\"üìÅ Repository: https://github.com/nimallansa937/HIMARI-LAYER-3-POSITIONING-\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rl.trainer import RLTrainer, TrainingConfig\n",
    "from rl.trading_env import EnvConfig\n",
    "from rl.ppo_agent import PPOConfig\n",
    "\n",
    "print(f\"‚úÖ Imports successful\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üîß Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = TrainingConfig(\n",
    "    num_episodes=1000,           # Number of training episodes\n",
    "    max_steps_per_episode=500,   # Max steps per episode\n",
    "    batch_size=64,               # Batch size for PPO updates\n",
    "    ppo_epochs=10,               # PPO optimization epochs\n",
    "    save_interval=50,            # Save checkpoint every N episodes\n",
    "    log_interval=10,             # Log progress every N episodes\n",
    "    checkpoint_dir='checkpoints', # Checkpoint directory\n",
    "    use_live_prices=True         # Use live Binance prices\n",
    ")\n",
    "\n",
    "# Environment configuration\n",
    "env_config = EnvConfig(\n",
    "    initial_capital=100000.0,    # Starting capital\n",
    "    max_position_pct=0.5,        # Max 50% per position\n",
    "    commission_rate=0.001,       # 0.1% commission\n",
    "    slippage_bps=5,              # 0.05% slippage\n",
    "    reward_window=10,            # Sharpe calculation window\n",
    "    max_steps=500,               # Max steps per episode\n",
    "    symbol='BTC-USD'             # Trading symbol\n",
    ")\n",
    "\n",
    "# PPO agent configuration\n",
    "ppo_config = PPOConfig(\n",
    "    state_dim=16,                # State dimension\n",
    "    action_dim=1,                # Action dimension (position multiplier)\n",
    "    hidden_dim=128,              # Hidden layer size\n",
    "    learning_rate=3e-4,          # Learning rate\n",
    "    gamma=0.99,                  # Discount factor\n",
    "    lambda_gae=0.95,             # GAE parameter\n",
    "    clip_epsilon=0.2,            # PPO clip parameter\n",
    ")\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"  Episodes: {training_config.num_episodes}\")\n",
    "print(f\"  Initial Capital: ${env_config.initial_capital:,.0f}\")\n",
    "print(f\"  Symbol: {env_config.symbol}\")\n",
    "print(f\"  Live Prices: {training_config.use_live_prices}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = RLTrainer(\n",
    "    training_config=training_config,\n",
    "    env_config=env_config,\n",
    "    ppo_config=ppo_config,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")\n",
    "print(f\"  Environment: {trainer.env.__class__.__name__}\")\n",
    "print(f\"  Agent: {trainer.agent.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "training_stats = trainer.train()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(training_stats['episode_rewards'])\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Sharpe ratio\n",
    "axes[0, 1].plot(training_stats['episode_sharpes'])\n",
    "axes[0, 1].set_title('Episode Sharpe Ratio')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Sharpe Ratio')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# P&L percentage\n",
    "axes[1, 0].plot(np.array(training_stats['episode_pnls']) * 100)\n",
    "axes[1, 0].set_title('Episode P&L %')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('P&L %')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Episode lengths\n",
    "axes[1, 1].plot(training_stats['episode_lengths'])\n",
    "axes[1, 1].set_title('Episode Lengths')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Steps')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Training curves saved to training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate agent\n",
    "print(\"üß™ Evaluating trained agent...\")\n",
    "eval_results = trainer.evaluate(num_episodes=20)\n",
    "\n",
    "print(\"\\nüìà Evaluation Results:\")\n",
    "print(f\"  Average Reward:    {eval_results['avg_reward']:.3f} ¬± {eval_results['std_reward']:.3f}\")\n",
    "print(f\"  Average Sharpe:    {eval_results['avg_sharpe']:.3f} ¬± {eval_results['std_sharpe']:.3f}\")\n",
    "print(f\"  Average P&L:       {eval_results['avg_pnl']:.2%}\")\n",
    "print(f\"  Average Win Rate:  {eval_results['avg_win_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = 'checkpoints/ppo_final.pt'\n",
    "trainer.agent.save(final_model_path)\n",
    "\n",
    "print(f\"üíæ Final model saved to: {final_model_path}\")\n",
    "print(\"\\nüì¶ To download:\")\n",
    "print(\"  1. Right-click on file in Colab file browser\")\n",
    "print(\"  2. Select 'Download'\")\n",
    "print(\"  3. Place in HIMARI/LAYER 3 POSITIONING LAYER/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with random state\n",
    "print(\"üî¨ Testing inference...\")\n",
    "\n",
    "test_state = np.random.randn(16).astype(np.float32)\n",
    "action, log_prob = trainer.agent.get_action(test_state, deterministic=True)\n",
    "\n",
    "print(f\"  Test state shape: {test_state.shape}\")\n",
    "print(f\"  Output action:    {action:.3f} (position multiplier)\")\n",
    "print(f\"  Valid range:      [0.0, 2.0]\")\n",
    "print(f\"  Status:           {'‚úÖ PASS' if 0 <= action <= 2 else '‚ùå FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Training Complete! üéâ**\n",
    "\n",
    "Next steps:\n",
    "1. Download the trained model (`ppo_final.pt`)\n",
    "2. Place it in your local HIMARI directory\n",
    "3. Use `Layer3Phase1RL` with `rl_model_path='models/ppo_final.pt'`\n",
    "4. Deploy to paper trading\n",
    "\n",
    "**Key Files:**\n",
    "- `checkpoints/ppo_final.pt` - Trained model weights\n",
    "- `checkpoints/stats_episode_*.json` - Training statistics\n",
    "- `training_curves.png` - Performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Auto-save important files to Google Drive\nprint(\"üíæ Saving files to Google Drive...\")\n\nfrom google.colab import drive\nimport shutil\nimport os\n\n# Mount Google Drive\ndrive.mount('/content/drive', force_remount=True)\n\n# Create save directory\nsave_dir = '/content/drive/MyDrive/HIMARI_RL_Models/'\nos.makedirs(save_dir, exist_ok=True)\n\n# Save final model\nshutil.copy('checkpoints/ppo_final.pt', f'{save_dir}ppo_final.pt')\nprint(f\"‚úÖ Model saved: {save_dir}ppo_final.pt\")\n\n# Save training curves\nshutil.copy('training_curves.png', f'{save_dir}training_curves.png')\nprint(f\"‚úÖ Curves saved: {save_dir}training_curves.png\")\n\n# Save all checkpoints\ncheckpoint_save = f'{save_dir}checkpoints/'\nos.makedirs(checkpoint_save, exist_ok=True)\nfor file in os.listdir('checkpoints/'):\n    if file.endswith('.pt') or file.endswith('.json'):\n        shutil.copy(f'checkpoints/{file}', f'{checkpoint_save}{file}')\n\nprint(f\"‚úÖ Checkpoints saved: {checkpoint_save}\")\nprint(\"\\nüéâ All files backed up to Google Drive!\")\nprint(\"‚úÖ Safe to close Colab now - your work is saved!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Auto-Save to Google Drive (Recommended)",
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}